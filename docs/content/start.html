<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><title>Cloud9: A MapReduce Library for Hadoop</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="../style.css" type="text/css" />
</head>

<body>

<center><table width="80%"><tbody><tr><td align="left">

<h2>Cloud<sup><small>9</small></sup>: Getting started</h2>

<p>by Jimmy Lin</p>

<div class="main">

<p>This brief guide will get you started with
Cloud<sup><small>9</small></sup> in Eclipse.  A few notes to begin
with:</p>

<ul>

  <li>Instructions are written for the IBM cluster, but can be easily
  adapted to any other Hadoop cluster.  See postscript at very end if
  you want to play with a local one-node Hadoop cluster.</li>

  <li>The Cloud<sup><small>9</small></sup> demos have been verified to
  work in Windows (Java 1.5.0_02, Java 1.6.0_03) and Mac OS X 10.5
  (Java 1.5.0_13).</li>

  <li>If you're using Java 1.6, you must change your compiler
  compliance level to 5.0 or else your code is not going to run on the
  IBM cluster.  To do so, open Eclipse preferences (under Windows:
  Window &gt; Preferences; under Mac: Eclipse &gt; Preferences);
  select option Java &gt; Compiler &gt; Compiler compliance
  level.</li>

</ul>

<h3>Step 0: Download Various Software Packages</h3>

<p>Download and install the following software packages:</p>

<ul>

<li><a href="http://java.sun.com">Java</a>: version 1.6 is fine, but
see notes above.</li>

<li><a href="http://www.eclipse.org/">Eclipse</a>: an IDE for
Java.</li>

<li><a href="http://subclipse.tigris.org/">Subclipse</a>: a Subversion
client plug-in for Eclipse.</li>

</ul>

<h3>Step 1: Check Out Subversion Repositories</h3>

<p>Start Eclipse.  You'll to have tweak Subclipse options.  Open
Eclipse preferences (under Windows: Window &gt; Preferences; under
Mac: Eclipse &gt; Preferences).  Select option Team &gt; SVN. Change
SVN interface to "SVNKit".</p>

<p>Switch to repository exploring mode.  To do so, select menu option:
Window &gt; Open Perspective &gt; Other &gt; SVN Repository
Exploring.</p>

<p>Add repository by right clicking on left panel &gt; New &gt;
Repository Location. The two repositories you want to check out
are:</p>

<ul>

  <li><code>umd-hadoop-dist</code>: <a href="https://subversion.umiacs.umd.edu/umd-hadoop/dist">https://subversion.umiacs.umd.edu/umd-hadoop/dist</a></li>

  <li><code>umd-hadoop-core</code>: <a href="https://subversion.umiacs.umd.edu/umd-hadoop/core">https://subversion.umiacs.umd.edu/umd-hadoop/core</a></li>

</ul>

<p>For each repository, expand the tree.  You should see "branches",
"tags", and "trunk".  Right click on trunk &gt; Checkout...  Follow
dialog to check out repository.</p>

<p>When the checkout process is complete, switch back to the Java
perspective, and you should have two new projects:
<code>umd-hadoop-core</code> and <code>umd-hadoop-dist</code>.  If
you're still getting errors, you might need to recompile the projects.
Select menu option: Project &gt; Clean...</p>

<p>You'll note that Hadoop itself is checked in the
repository <code>umd-hadoop-dist</code>.  This is intentional, to
ensure everyone is using the same version of Hadoop (makes debugging
easier).  In the future, newer versions can be seamlessly rolled out
by checking in a newer version in Subversion and having everyone
update their sandboxes.</p>

<h3>Step 2: Install the Hadoop Eclipse Plug-In</h3>

<p>The next step is to install the Hadoop Eclipse plug-in.  Go to
directory
<code>umd-hadoop-dist/hadoop-0.15.3/contrib</code> and find the file 
<code>hadoop-0.15.3-eclipse-plugin.jar</code>.  Copy that file to
the <code>plugins</code> directory in your Eclipse directory.  Restart
Eclipse.</p>

<p>Restart Eclipse.  The Eclipse Hadoop plugin should now be
installed.</p>

<ul>
  <li>To use the MapReduce perspective go to: Window &gt; Open
Perspective &gt; Other... &gt; MapReduce.</li>

  <li>To enable the MapReduce servers window go to: Window &gt; Show
  View &gt; Other... &gt; MapReduce Tools &gt; MapReduce Servers</li>

</ul>

<h3>Step 3: Connect to the IBM Cluster</h3>

<p>Switch to the MapReduce perspective.  At the bottom of your window,
you should have a "MapReduce Servers" tab.  If not, see second bullet
above.  Switch to that tab.</p>

<p>At the top right edge of the tab, you should see two little blue
elephant icons.  The one on the right allows you to add a new
MapReduce server location.  The hostname should be the IP address of
the controller.  You want to enable "Tunnel Connections" and put in
the IP address of the gateway.</p>

<p>At this point, you should now have access to DFS.  It should show
up under a little elephant icon in the Project Explorer (on the left
side of Eclipse).  You can now browse the directory tree.  Your home
directory should be <code>/user/your_username</code>.  A sample
collection consisting of the Bible and Shakespeare's works has been
preloaded on the cluster, stored
at <code>/shared/sample-input</code>.</p>

<p>If you don't have access to the IBM cluster, you can download an
image of a single-node Hadoop cluster to play around with; see
instructions at the bottom of the page.</p>

<h3>Step 4: Run the Demos</h3>

<p>Find <code>edu.umd.cloud9.demo.DemoWordCount</code> in the Project
Explorer (panel on left in Eclipse).  This is your standard "Hello
World" Hadoop program that does word count.  Right click on the class,
select Run As &gt; Run on Hadoop.  This should pull up a dialog box;
select the Hadoop Server and watch it go!</p>

<p>When the job finishes, you should have something like the
following:</p>

<pre>
08/01/26 14:42:56 INFO mapred.JobClient:   Map-Reduce Framework
08/01/26 14:42:56 INFO mapred.JobClient:     Map input records=156215
08/01/26 14:42:56 INFO mapred.JobClient:     Map output records=1734298
08/01/26 14:42:56 INFO mapred.JobClient:     Map input bytes=9068074
08/01/26 14:42:56 INFO mapred.JobClient:     Map output bytes=15919397
08/01/26 14:42:56 INFO mapred.JobClient:     Combine input records=1734298
08/01/26 14:42:56 INFO mapred.JobClient:     Combine output records=135372
08/01/26 14:42:56 INFO mapred.JobClient:     Reduce input groups=41788
08/01/26 14:42:56 INFO mapred.JobClient:     Reduce input records=135372
08/01/26 14:42:56 INFO mapred.JobClient:     Reduce output records=41788
</pre>

<p>Go into your DFS home directory, and you should find a new
directory called <code>sample-counts</code>: in it you'll find files
containing counts of each unique word.  Congratulations, you're up and
running!</p>

<h3>Postscript</h3>

<p>Don't want to depend on the IBM cluster, especially while you are
tinkering around with small programs?  You can run a personal one-node
Hadoop cluster on your local machine: instructions
at <a href="http://code.google.com/edu/parallel/tools/hadoopvm/index.html">provided
by Google</a>.</p>

<p>To better understand the directory layout of the project, consult
page on <a href="../info/layout.html">layout of project directory
tree</a>.</p>

<!--

<h3>Adding Cloud<sup><small>9</small></sup> to your Eclipse project</h3>

<ol>

<li><p>First, create a Classpath Variable in Eclipse<br/>
Window -> Preferences -> Java -> Build Path -> Classpath Variable</p>

<p>Set the variable <code>ECLIPSE_WORKSPACE</code> to the absolute
path of your workspace.</p></li>

<li><p>Right click on the project you want to use
Cloud<sup><small>9</small></sup> in, select "Properties". Go to "Java
Build Path", go to "Source" table, click "Link Source..." button.
Type in the following:</p>

<blockquote>Linked folder location: ECLIPSE_WORKSPACE/umd-hadoop-core/src<br/>
Folder name: cloud9
</blockquote>

<p>Click "Finish".</p>
</li>

<li><p>You should now be able to use Cloud<sup><small>9</small></sup> in
your Eclipse project!</p></li>

</ol>

<p>Some explanation on this process:</p>

<ul>

<li>You have to directly link in the
source to retain compatability with the Hadoop Eclipse plug-in.
Although in Eclipse you can simply include a jar in the build path,
the Hadoop Eclipse plug-in does not seem to correctly package up those
dependent jars and ship over to the cluster (so the code will compile
locally, but will fail to run on the remote cluster).</li>

<li>Setting <code>ECLIPSE_WORKSPACE</code> allows you to safely
check-in your code in Subversion &mdash; otherwise, encoding an absolute
path will break to repository for other users, since everyone is
likely to have their workspace in a different place.</li>

</ul>

-->

<p style="padding-top: 25px"><a href="../index.html">Back to main page</a></p>

</div>

<table width="100%" border="0" cellpadding="0" cellspacing="0" style="padding-top: 10px;">
<tr><td valign="top" align="left">
<small>This page, first created: 30 Oct 2007; last updated:
<script language="JavaScript" type="text/javascript">
<!--
var LastUpdated = "$Date$";
LastUpdated = LastUpdated.substring(LastUpdated.length-14, LastUpdated.length-3);
document.writeln (LastUpdated);
-->
</script>
</small>
</td>
<td valign="top" align="right">
  <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/us/">
  <img src="../images/creative-commons.png" border="0" alt="Creative Commons: Attribution-Noncommercial-Share Alike 3.0 United States"/>
  </a>
  <a href="http://validator.w3.org/check/referer">
  <img src="../images/valid-xhtml10.gif" border="0"
       alt="Valid XHTML 1.0!" height="31" width="88" />
  </a>
  <a href="http://jigsaw.w3.org/css-validator/check/referer">
  <img style="border:0;width:88px;height:31px"
       src="../images/vcss.gif" 
       alt="Valid CSS!" />
  </a>
</td></tr></table>

</td></tr></tbody></table></center>

</body></html>
