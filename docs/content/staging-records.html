<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><title>Cloud9: A MapReduce Library for Hadoop</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="../style.css" type="text/css" />
</head>

<body>

<center><table width="80%"><tbody><tr><td align="left">

<h2>Cloud<sup><small>9</small></sup>: Staging records and working with SequenceFiles</h2>

<p>by Jimmy Lin</p>

<p>
<small>(Page first created: 30 Oct 2007; last updated:
<script language="JavaScript" type="text/javascript">
<!--
var LastUpdated = "$Date$";
LastUpdated = LastUpdated.substring(LastUpdated.length-14, LastUpdated.length-3);
document.writeln (LastUpdated+")");
-->
</script>
</small>
</p>

<div class="main">

<h3>Introduction</h3>

<p>All MapReduce jobs begin with a collection of records, which
consists of the initial key-value pairs passed to the mapper.  The
Hadoop InputFormat interface describes this input specification.
Simple implementations such as TextInputFormat assume records are text
blobs stored in a file, one line per record.  This is obviously
insufficient for more complex record structures.  One solution is keep
input records in some type of text-based structured format (e.g., XML)
and write record readers.  The downside is the need to write custom
code for every record type.  The other downside is that you'll have to
reparse the text-based input every time.</p>

<p>Once you've become acquainted with the word count demo, you'll want
to next learn about SequenceFiles in Hadoop.  They are flat files that
contain a sequence of key-value pairs encoded in a binary format.
When reading a SequenceFile, Hadoop automatically deserializes the
keys and values for you.  Therefore, the most straightforward solution
for preparing input to MapReduce jobs is to preprocess your data to
create SequenceFiles.  The typical sequence of actions might be:</p>

<ol>

<li>Create SequenceFiles from your input data on your local machine.</li>
<li>Copy the SequenceFiles over to the cluster (via "scp").</li>
<li>Copy the SequenceFiles into HDFS (via "hadoop dfs -put").</li>
<li>Start MapReducing!</li>

</ol>

<p>Creating a SequenceFile is pretty straightforward:</p>

<pre>
Configuration conf = new Configuration();
FileSystem fs = FileSystem.get(conf);
SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, new Path(outfile),
    LongWritable.class, JSONObjectWritable.class);
</pre>

<p>In this case, outfile is the name of the output file.  The key type
in the SequenceFile is LongWritable, and the value type is
JSONObjectWritable.  Once you've created the writer, adding key-value
pairs is easy:</p>

<pre>
LongWritable l = new LongWritable();
JSONObjectWritable json = new JSONObjectWritable();
...

writer.append(l, json);
</pre>

<p>Remember to close the file when you are done!</p>

<pre>
writer.close();
</pre>

<h3>Reading SequenceFiles</h3>

<p>Use the following code fragment for reading SequenceFiles:</p>

<pre>
Configuration config = new Configuration();
FileSystem fs = FileSystem.get(config);
Path path = new Path("/path/to/file");
SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, config);

WritableComparable key = (WritableComparable) reader.getKeyClass().newInstance();
Writable value = (Writable) reader.getValueClass().newInstance();

while (reader.next(key, value)) {
    // do something
}
reader.close();
</pre>

<p>Or easier still, use SequenceFileUtils in
Cloud<sup><small>9</small></sup> (for example, reading in key-value
pairs with IntWritable keys and JSONObjectWritable values):</p>

<pre>
List&lt;KeyValuePair&lt;IntWritable, JSONObjectWritable&gt;&gt; pairs = 
    SequenceFileUtils.&lt;IntWritable, JSONObjectWritable&gt;readDirectory(path, Integer.MAX_VALUE);

for ( KeyValuePair&lt;IntWritable, JSONObjectWritable&gt; pair : pairs ) {
    // do something
}
</pre>

<p>First argument is the path, the second is the maximum number of
entries to read.</p>

<h3>Windows users beware!</h3>

<p>On Windows (Cygwin), Hadoop may croak with the following error when
writing SequenceFiles:</p>

<pre>
08/08/11 08:55:26 WARN fs.FileSystem: uri=file:///
javax.security.auth.login.LoginException: Login failed: Expect one token as the result of whoami: Jimmy Lin
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:250)
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:275)
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:257)
        at org.apache.hadoop.security.UserGroupInformation.login(UserGroupInformation.java:67)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.&lt;init&gt;(FileSystem.java:1353)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1289)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:203)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:108)
        at edu.umd.cloud9.demo.DemoPackRecords.main(DemoPackRecords.java:69)
08/08/11 08:55:26 WARN fs.FileSystem: uri=file:///
javax.security.auth.login.LoginException: Login failed: Expect one token as the result of whoami: Jimmy Lin
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:250)
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:275)
        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:257)
        at org.apache.hadoop.security.UserGroupInformation.login(UserGroupInformation.java:67)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.&lt;init&gt;(FileSystem.java:1353)
        at org.apache.hadoop.fs.FileSystem$Cache$Key.&lt;init&gt;(FileSystem.java:1344)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1295)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:203)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:108)
        at edu.umd.cloud9.demo.DemoPackRecords.main(DemoPackRecords.java:69)
</pre>

<p>The problem is that Windows usernames contain spaces, which Hadoop
does not expect.  To fix this, in Cygwin edit the /etc/passwd file and
change your username (first field).</p>

<p style="padding-top: 25px"><a href="../index.html">Back to main page</a></p>

</div>

<table width="100%" border="0" cellpadding="0" cellspacing="0" style="padding-top: 10px;">
<tr><td valign="top" align="left">
</td>
<td valign="top" align="right">
  <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/us/">
  <img src="../images/creative-commons.png" border="0" alt="Creative Commons: Attribution-Noncommercial-Share Alike 3.0 United States"/>
  </a>
  <a href="http://validator.w3.org/check/referer">
  <img src="../images/valid-xhtml10.gif" border="0"
       alt="Valid XHTML 1.0!" height="31" width="88" />
  </a>
  <a href="http://jigsaw.w3.org/css-validator/check/referer">
  <img style="border:0;width:88px;height:31px"
       src="../images/vcss.gif" 
       alt="Valid CSS!" />
  </a>
</td></tr></table>

</td></tr></tbody></table></center>

</body></html>
